{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lab5.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UHX9p5jfTySS"
      },
      "source": [
        "## Задание 5.1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0EnHNZtbXlH0"
      },
      "source": [
        "Набор данных тут: https://github.com/sismetanin/rureviews, также есть в папке [Data](https://drive.google.com/drive/folders/1YAMe7MiTxA-RSSd8Ex2p-L0Dspe6Gs4L). Те, кто предпочитает работать с английским языком, могут использовать набор данных `sms_spam`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xASL4mLKDn_z",
        "outputId": "41300dd1-943b-43f5-91d5-ecefa418a0ed"
      },
      "source": [
        "!pip install pymorphy2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pymorphy2 in /usr/local/lib/python3.7/dist-packages (0.9.1)\n",
            "Requirement already satisfied: pymorphy2-dicts-ru<3.0,>=2.4 in /usr/local/lib/python3.7/dist-packages (from pymorphy2) (2.4.417127.4579844)\n",
            "Requirement already satisfied: docopt>=0.6 in /usr/local/lib/python3.7/dist-packages (from pymorphy2) (0.6.2)\n",
            "Requirement already satisfied: dawg-python>=0.7.1 in /usr/local/lib/python3.7/dist-packages (from pymorphy2) (0.7.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5karayYrGDYU"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import math\n",
        "import nltk \n",
        "import string"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i_wtwWt7xwEO"
      },
      "source": [
        "from sklearn.metrics import * \n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZkraR4lAGKV7"
      },
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WymMQvZRGYg_",
        "outputId": "cd2eb796-bdaf-471d-fb8f-e45e7c12b36d"
      },
      "source": [
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J2MsUyqbGNkj"
      },
      "source": [
        "from pymorphy2 import MorphAnalyzer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z8j95S6qxS3o",
        "outputId": "e458e1c4-c807-406f-eb4a-efc90de47a2c"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "coVAmOhLxrmH"
      },
      "source": [
        "data = pd.read_csv(\"/content/drive/MyDrive/Data/women-clothing-accessories.csv\", sep=\"\\t\", usecols=[0, 1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y6X0nNyYyL6x"
      },
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(data.review, data.sentiment, train_size = 0.7)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PMWAMt6MoGvJ"
      },
      "source": [
        "new_data = data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bJox-LoonoPx"
      },
      "source": [
        "Применим полученные навыки и решим задачу анализа тональности отзывов. \n",
        "\n",
        "Нужно повторить весь пайплайн от сырых текстов до получения обученной модели.\n",
        "\n",
        "Обязательные шаги предобработки:\n",
        "1. токенизация\n",
        "2. приведение к нижнему регистру\n",
        "3. удаление стоп-слов\n",
        "4. лемматизация\n",
        "5. векторизация (с настройкой гиперпараметров)\n",
        "6. построение модели\n",
        "7. оценка качества модели\n",
        "\n",
        "Обязательно использование векторайзеров:\n",
        "1. мешок n-грамм (диапазон для n подбирайте самостоятельно, запрещено использовать только униграммы).\n",
        "2. tf-idf ((диапазон для n подбирайте самостоятельно, также нужно подбирать параметры max_df, min_df, max_features)\n",
        "3. символьные n-граммы (диапазон для n подбирайте самостоятельно)\n",
        "\n",
        "В качестве классификатора нужно использовать наивный байесовский классификатор. \n",
        "\n",
        "Для сравнения векторайзеров между собой используйте precision, recall, f1-score и accuracy. Для этого сформируйте датафрейм, в котором в строках будут разные векторайзеры, а в столбцах разные метрики качества, а в  ячейках будут значения этих метрик для соответсвующих векторайзеров."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8N4EzZoZEfEA"
      },
      "source": [
        "def lemmatization(data):\n",
        "  morph_analyzer = MorphAnalyzer()\n",
        "  sentences = []\n",
        "  for sentence in data:\n",
        "    for symbol in string.punctuation:\n",
        "      sentence = sentence.replace(symbol, \" \")\n",
        "    processed_sentences = [morph_analyzer.parse(word)[0].normal_form.lower() for word in word_tokenize(sentence)]\n",
        "    sentences.append(\" \".join(processed_sentences))\n",
        "  return sentences"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NU0PtYe-y9Oj"
      },
      "source": [
        "new_data.review = lemmatization(data.review)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WgsWDhVedU9A"
      },
      "source": [
        "my_x_train, my_x_test, my_y_train, my_y_test = train_test_split(data.review, data.sentiment, train_size = 0.7)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ojp_S0A8mXME"
      },
      "source": [
        "vectorize_result = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kYbAB8OVT0Tx"
      },
      "source": [
        "def fit_transform(vectorizer):\n",
        "  return vectorizer, vectorizer.fit_transform(my_x_train)\n",
        "\n",
        "def generator(from_min_n, to_min_n, to_max_n):\n",
        "  ngram_parameters = []\n",
        "  for min_n in range(from_min_n, to_min_n):\n",
        "    for max_n in range(min_n, to_max_n):\n",
        "      ngram_parameters.append((min_n, max_n))\n",
        "  return list(ngram_parameters)\n",
        "\n",
        "def naive_bayes(vectorizer, vectorizer_x_train):\n",
        "  my_clf = MultinomialNB()\n",
        "  my_clf.fit(vectorizer_x_train, my_y_train)  \n",
        "  vectorizer_x_test = vectorizer.transform(my_x_test)\n",
        "  my_pred = my_clf.predict(vectorizer_x_test)\n",
        "  print(\"ngram_range:\", str(vectorizer.ngram_range), \"analyzer:\", str(vectorizer.analyzer))\n",
        "  print(classification_report(my_y_test, my_pred, output_dict=False))\n",
        "  return classification_report(my_y_test, my_pred, output_dict=True), vectorizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s88twcQNmUY4"
      },
      "source": [
        "###CountVectorizer word analyzer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D2pQEDobU6Qe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98b5e801-fb65-492b-a0c7-578c80373eff"
      },
      "source": [
        "for ngram_range in generator(2, 6, 6):\n",
        "  count_vectorizer, count_vectorizer_x_train = fit_transform(CountVectorizer(ngram_range=ngram_range, stop_words=stopwords.words(\"russian\"), analyzer=\"word\"))\n",
        "  vectorize_result.append(naive_bayes(count_vectorizer, count_vectorizer_x_train))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ngram_range: (2, 2) analyzer: word\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    neautral       0.59      0.54      0.56      8980\n",
            "    negative       0.71      0.65      0.68      9126\n",
            "    positive       0.74      0.86      0.80      8895\n",
            "\n",
            "    accuracy                           0.68     27001\n",
            "   macro avg       0.68      0.69      0.68     27001\n",
            "weighted avg       0.68      0.68      0.68     27001\n",
            "\n",
            "ngram_range: (2, 3) analyzer: word\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    neautral       0.59      0.52      0.55      8980\n",
            "    negative       0.70      0.66      0.68      9126\n",
            "    positive       0.73      0.87      0.79      8895\n",
            "\n",
            "    accuracy                           0.68     27001\n",
            "   macro avg       0.67      0.68      0.68     27001\n",
            "weighted avg       0.67      0.68      0.67     27001\n",
            "\n",
            "ngram_range: (2, 4) analyzer: word\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    neautral       0.59      0.51      0.55      8980\n",
            "    negative       0.70      0.66      0.68      9126\n",
            "    positive       0.73      0.86      0.79      8895\n",
            "\n",
            "    accuracy                           0.68     27001\n",
            "   macro avg       0.67      0.68      0.67     27001\n",
            "weighted avg       0.67      0.68      0.67     27001\n",
            "\n",
            "ngram_range: (2, 5) analyzer: word\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    neautral       0.59      0.51      0.55      8980\n",
            "    negative       0.70      0.66      0.68      9126\n",
            "    positive       0.73      0.86      0.79      8895\n",
            "\n",
            "    accuracy                           0.68     27001\n",
            "   macro avg       0.67      0.68      0.67     27001\n",
            "weighted avg       0.67      0.68      0.67     27001\n",
            "\n",
            "ngram_range: (3, 3) analyzer: word\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    neautral       0.52      0.30      0.38      8980\n",
            "    negative       0.69      0.54      0.61      9126\n",
            "    positive       0.53      0.87      0.66      8895\n",
            "\n",
            "    accuracy                           0.57     27001\n",
            "   macro avg       0.58      0.57      0.55     27001\n",
            "weighted avg       0.58      0.57      0.55     27001\n",
            "\n",
            "ngram_range: (3, 4) analyzer: word\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    neautral       0.51      0.30      0.38      8980\n",
            "    negative       0.69      0.54      0.60      9126\n",
            "    positive       0.53      0.86      0.66      8895\n",
            "\n",
            "    accuracy                           0.57     27001\n",
            "   macro avg       0.58      0.57      0.55     27001\n",
            "weighted avg       0.58      0.57      0.55     27001\n",
            "\n",
            "ngram_range: (3, 5) analyzer: word\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    neautral       0.51      0.30      0.38      8980\n",
            "    negative       0.69      0.54      0.60      9126\n",
            "    positive       0.53      0.86      0.66      8895\n",
            "\n",
            "    accuracy                           0.57     27001\n",
            "   macro avg       0.58      0.57      0.55     27001\n",
            "weighted avg       0.58      0.57      0.55     27001\n",
            "\n",
            "ngram_range: (4, 4) analyzer: word\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    neautral       0.45      0.11      0.18      8980\n",
            "    negative       0.70      0.31      0.43      9126\n",
            "    positive       0.39      0.92      0.55      8895\n",
            "\n",
            "    accuracy                           0.45     27001\n",
            "   macro avg       0.52      0.45      0.39     27001\n",
            "weighted avg       0.52      0.45      0.39     27001\n",
            "\n",
            "ngram_range: (4, 5) analyzer: word\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    neautral       0.45      0.11      0.18      8980\n",
            "    negative       0.70      0.31      0.43      9126\n",
            "    positive       0.39      0.92      0.55      8895\n",
            "\n",
            "    accuracy                           0.45     27001\n",
            "   macro avg       0.51      0.45      0.39     27001\n",
            "weighted avg       0.52      0.45      0.39     27001\n",
            "\n",
            "ngram_range: (5, 5) analyzer: word\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    neautral       0.40      0.03      0.05      8980\n",
            "    negative       0.75      0.12      0.20      9126\n",
            "    positive       0.35      0.98      0.51      8895\n",
            "\n",
            "    accuracy                           0.37     27001\n",
            "   macro avg       0.50      0.37      0.26     27001\n",
            "weighted avg       0.50      0.37      0.25     27001\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "36U4f-j6vY15"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tsgdjPKPveWp"
      },
      "source": [
        "###TfidfVectorizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QWrgDkmqvgcI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "411bc42b-8bf2-46f0-e630-87ebf8ec2b31"
      },
      "source": [
        "for ngram_range in generator(2, 4, 4):\n",
        "  for max_df in [0.1, 0.5]:\n",
        "    for min_df in [0, 0.01]:\n",
        "      for max_features in [2048, 8192, 32768]:\n",
        "        print(max_df, min_df)\n",
        "        tfidf_vectorizer, tfidf_vectorizer_x_train = fit_transform(TfidfVectorizer(ngram_range=ngram_range, max_df=max_df, min_df=min_df, max_features=max_features))\n",
        "        vectorize_result.append(naive_bayes(tfidf_vectorizer, tfidf_vectorizer_x_train))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.1 0\n",
            "ngram_range: (2, 2) analyzer: word\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    neautral       0.57      0.61      0.59      8980\n",
            "    negative       0.74      0.59      0.66      9126\n",
            "    positive       0.74      0.84      0.79      8895\n",
            "\n",
            "    accuracy                           0.68     27001\n",
            "   macro avg       0.68      0.68      0.68     27001\n",
            "weighted avg       0.68      0.68      0.68     27001\n",
            "\n",
            "0.1 0\n",
            "ngram_range: (2, 2) analyzer: word\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    neautral       0.59      0.64      0.62      8980\n",
            "    negative       0.74      0.62      0.68      9126\n",
            "    positive       0.80      0.85      0.82      8895\n",
            "\n",
            "    accuracy                           0.70     27001\n",
            "   macro avg       0.71      0.71      0.70     27001\n",
            "weighted avg       0.71      0.70      0.70     27001\n",
            "\n",
            "0.1 0\n",
            "ngram_range: (2, 2) analyzer: word\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    neautral       0.60      0.65      0.62      8980\n",
            "    negative       0.74      0.64      0.68      9126\n",
            "    positive       0.82      0.86      0.84      8895\n",
            "\n",
            "    accuracy                           0.71     27001\n",
            "   macro avg       0.72      0.72      0.71     27001\n",
            "weighted avg       0.72      0.71      0.71     27001\n",
            "\n",
            "0.1 0.01\n",
            "ngram_range: (2, 2) analyzer: word\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    neautral       0.51      0.37      0.43      8980\n",
            "    negative       0.74      0.47      0.58      9126\n",
            "    positive       0.53      0.87      0.66      8895\n",
            "\n",
            "    accuracy                           0.57     27001\n",
            "   macro avg       0.59      0.57      0.55     27001\n",
            "weighted avg       0.60      0.57      0.55     27001\n",
            "\n",
            "0.1 0.01\n",
            "ngram_range: (2, 2) analyzer: word\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    neautral       0.51      0.37      0.43      8980\n",
            "    negative       0.74      0.47      0.58      9126\n",
            "    positive       0.53      0.87      0.66      8895\n",
            "\n",
            "    accuracy                           0.57     27001\n",
            "   macro avg       0.59      0.57      0.55     27001\n",
            "weighted avg       0.60      0.57      0.55     27001\n",
            "\n",
            "0.1 0.01\n",
            "ngram_range: (2, 2) analyzer: word\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    neautral       0.51      0.37      0.43      8980\n",
            "    negative       0.74      0.47      0.58      9126\n",
            "    positive       0.53      0.87      0.66      8895\n",
            "\n",
            "    accuracy                           0.57     27001\n",
            "   macro avg       0.59      0.57      0.55     27001\n",
            "weighted avg       0.60      0.57      0.55     27001\n",
            "\n",
            "0.5 0\n",
            "ngram_range: (2, 2) analyzer: word\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    neautral       0.57      0.61      0.59      8980\n",
            "    negative       0.74      0.59      0.66      9126\n",
            "    positive       0.74      0.84      0.79      8895\n",
            "\n",
            "    accuracy                           0.68     27001\n",
            "   macro avg       0.68      0.68      0.68     27001\n",
            "weighted avg       0.68      0.68      0.68     27001\n",
            "\n",
            "0.5 0\n",
            "ngram_range: (2, 2) analyzer: word\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    neautral       0.59      0.64      0.62      8980\n",
            "    negative       0.74      0.62      0.68      9126\n",
            "    positive       0.80      0.85      0.82      8895\n",
            "\n",
            "    accuracy                           0.70     27001\n",
            "   macro avg       0.71      0.71      0.70     27001\n",
            "weighted avg       0.71      0.70      0.70     27001\n",
            "\n",
            "0.5 0\n",
            "ngram_range: (2, 2) analyzer: word\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    neautral       0.60      0.65      0.62      8980\n",
            "    negative       0.74      0.64      0.68      9126\n",
            "    positive       0.82      0.86      0.84      8895\n",
            "\n",
            "    accuracy                           0.71     27001\n",
            "   macro avg       0.72      0.72      0.71     27001\n",
            "weighted avg       0.72      0.71      0.71     27001\n",
            "\n",
            "0.5 0.01\n",
            "ngram_range: (2, 2) analyzer: word\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    neautral       0.51      0.37      0.43      8980\n",
            "    negative       0.74      0.47      0.58      9126\n",
            "    positive       0.53      0.87      0.66      8895\n",
            "\n",
            "    accuracy                           0.57     27001\n",
            "   macro avg       0.59      0.57      0.55     27001\n",
            "weighted avg       0.60      0.57      0.55     27001\n",
            "\n",
            "0.5 0.01\n",
            "ngram_range: (2, 2) analyzer: word\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    neautral       0.51      0.37      0.43      8980\n",
            "    negative       0.74      0.47      0.58      9126\n",
            "    positive       0.53      0.87      0.66      8895\n",
            "\n",
            "    accuracy                           0.57     27001\n",
            "   macro avg       0.59      0.57      0.55     27001\n",
            "weighted avg       0.60      0.57      0.55     27001\n",
            "\n",
            "0.5 0.01\n",
            "ngram_range: (2, 2) analyzer: word\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    neautral       0.51      0.37      0.43      8980\n",
            "    negative       0.74      0.47      0.58      9126\n",
            "    positive       0.53      0.87      0.66      8895\n",
            "\n",
            "    accuracy                           0.57     27001\n",
            "   macro avg       0.59      0.57      0.55     27001\n",
            "weighted avg       0.60      0.57      0.55     27001\n",
            "\n",
            "0.1 0\n",
            "ngram_range: (2, 3) analyzer: word\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    neautral       0.57      0.60      0.59      8980\n",
            "    negative       0.75      0.59      0.66      9126\n",
            "    positive       0.73      0.85      0.79      8895\n",
            "\n",
            "    accuracy                           0.68     27001\n",
            "   macro avg       0.68      0.68      0.68     27001\n",
            "weighted avg       0.68      0.68      0.68     27001\n",
            "\n",
            "0.1 0\n",
            "ngram_range: (2, 3) analyzer: word\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    neautral       0.58      0.64      0.61      8980\n",
            "    negative       0.75      0.61      0.67      9126\n",
            "    positive       0.78      0.85      0.81      8895\n",
            "\n",
            "    accuracy                           0.70     27001\n",
            "   macro avg       0.70      0.70      0.70     27001\n",
            "weighted avg       0.70      0.70      0.70     27001\n",
            "\n",
            "0.1 0\n",
            "ngram_range: (2, 3) analyzer: word\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    neautral       0.59      0.65      0.62      8980\n",
            "    negative       0.75      0.63      0.68      9126\n",
            "    positive       0.81      0.86      0.83      8895\n",
            "\n",
            "    accuracy                           0.71     27001\n",
            "   macro avg       0.72      0.71      0.71     27001\n",
            "weighted avg       0.72      0.71      0.71     27001\n",
            "\n",
            "0.1 0.01\n",
            "ngram_range: (2, 3) analyzer: word\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    neautral       0.51      0.37      0.43      8980\n",
            "    negative       0.75      0.46      0.57      9126\n",
            "    positive       0.53      0.88      0.66      8895\n",
            "\n",
            "    accuracy                           0.57     27001\n",
            "   macro avg       0.60      0.57      0.55     27001\n",
            "weighted avg       0.60      0.57      0.55     27001\n",
            "\n",
            "0.1 0.01\n",
            "ngram_range: (2, 3) analyzer: word\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    neautral       0.51      0.37      0.43      8980\n",
            "    negative       0.75      0.46      0.57      9126\n",
            "    positive       0.53      0.88      0.66      8895\n",
            "\n",
            "    accuracy                           0.57     27001\n",
            "   macro avg       0.60      0.57      0.55     27001\n",
            "weighted avg       0.60      0.57      0.55     27001\n",
            "\n",
            "0.1 0.01\n",
            "ngram_range: (2, 3) analyzer: word\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    neautral       0.51      0.37      0.43      8980\n",
            "    negative       0.75      0.46      0.57      9126\n",
            "    positive       0.53      0.88      0.66      8895\n",
            "\n",
            "    accuracy                           0.57     27001\n",
            "   macro avg       0.60      0.57      0.55     27001\n",
            "weighted avg       0.60      0.57      0.55     27001\n",
            "\n",
            "0.5 0\n",
            "ngram_range: (2, 3) analyzer: word\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    neautral       0.57      0.60      0.59      8980\n",
            "    negative       0.75      0.59      0.66      9126\n",
            "    positive       0.73      0.85      0.79      8895\n",
            "\n",
            "    accuracy                           0.68     27001\n",
            "   macro avg       0.68      0.68      0.68     27001\n",
            "weighted avg       0.68      0.68      0.68     27001\n",
            "\n",
            "0.5 0\n",
            "ngram_range: (2, 3) analyzer: word\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    neautral       0.58      0.64      0.61      8980\n",
            "    negative       0.75      0.61      0.67      9126\n",
            "    positive       0.78      0.85      0.81      8895\n",
            "\n",
            "    accuracy                           0.70     27001\n",
            "   macro avg       0.70      0.70      0.70     27001\n",
            "weighted avg       0.70      0.70      0.70     27001\n",
            "\n",
            "0.5 0\n",
            "ngram_range: (2, 3) analyzer: word\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    neautral       0.59      0.65      0.62      8980\n",
            "    negative       0.75      0.63      0.68      9126\n",
            "    positive       0.81      0.86      0.83      8895\n",
            "\n",
            "    accuracy                           0.71     27001\n",
            "   macro avg       0.72      0.71      0.71     27001\n",
            "weighted avg       0.72      0.71      0.71     27001\n",
            "\n",
            "0.5 0.01\n",
            "ngram_range: (2, 3) analyzer: word\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    neautral       0.51      0.37      0.43      8980\n",
            "    negative       0.75      0.46      0.57      9126\n",
            "    positive       0.53      0.88      0.66      8895\n",
            "\n",
            "    accuracy                           0.57     27001\n",
            "   macro avg       0.60      0.57      0.55     27001\n",
            "weighted avg       0.60      0.57      0.55     27001\n",
            "\n",
            "0.5 0.01\n",
            "ngram_range: (2, 3) analyzer: word\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    neautral       0.51      0.37      0.43      8980\n",
            "    negative       0.75      0.46      0.57      9126\n",
            "    positive       0.53      0.88      0.66      8895\n",
            "\n",
            "    accuracy                           0.57     27001\n",
            "   macro avg       0.60      0.57      0.55     27001\n",
            "weighted avg       0.60      0.57      0.55     27001\n",
            "\n",
            "0.5 0.01\n",
            "ngram_range: (2, 3) analyzer: word\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    neautral       0.51      0.37      0.43      8980\n",
            "    negative       0.75      0.46      0.57      9126\n",
            "    positive       0.53      0.88      0.66      8895\n",
            "\n",
            "    accuracy                           0.57     27001\n",
            "   macro avg       0.60      0.57      0.55     27001\n",
            "weighted avg       0.60      0.57      0.55     27001\n",
            "\n",
            "0.1 0\n",
            "ngram_range: (3, 3) analyzer: word\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    neautral       0.55      0.37      0.44      8980\n",
            "    negative       0.76      0.48      0.59      9126\n",
            "    positive       0.52      0.89      0.66      8895\n",
            "\n",
            "    accuracy                           0.58     27001\n",
            "   macro avg       0.61      0.58      0.56     27001\n",
            "weighted avg       0.61      0.58      0.56     27001\n",
            "\n",
            "0.1 0\n",
            "ngram_range: (3, 3) analyzer: word\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    neautral       0.55      0.44      0.49      8980\n",
            "    negative       0.75      0.54      0.63      9126\n",
            "    positive       0.58      0.87      0.70      8895\n",
            "\n",
            "    accuracy                           0.61     27001\n",
            "   macro avg       0.63      0.62      0.60     27001\n",
            "weighted avg       0.63      0.61      0.60     27001\n",
            "\n",
            "0.1 0\n",
            "ngram_range: (3, 3) analyzer: word\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    neautral       0.57      0.48      0.52      8980\n",
            "    negative       0.73      0.59      0.65      9126\n",
            "    positive       0.64      0.86      0.73      8895\n",
            "\n",
            "    accuracy                           0.64     27001\n",
            "   macro avg       0.65      0.64      0.64     27001\n",
            "weighted avg       0.65      0.64      0.64     27001\n",
            "\n",
            "0.1 0.01\n",
            "ngram_range: (3, 3) analyzer: word\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    neautral       0.42      0.02      0.03      8980\n",
            "    negative       0.82      0.20      0.32      9126\n",
            "    positive       0.36      0.99      0.53      8895\n",
            "\n",
            "    accuracy                           0.40     27001\n",
            "   macro avg       0.54      0.40      0.30     27001\n",
            "weighted avg       0.54      0.40      0.29     27001\n",
            "\n",
            "0.1 0.01\n",
            "ngram_range: (3, 3) analyzer: word\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    neautral       0.42      0.02      0.03      8980\n",
            "    negative       0.82      0.20      0.32      9126\n",
            "    positive       0.36      0.99      0.53      8895\n",
            "\n",
            "    accuracy                           0.40     27001\n",
            "   macro avg       0.54      0.40      0.30     27001\n",
            "weighted avg       0.54      0.40      0.29     27001\n",
            "\n",
            "0.1 0.01\n",
            "ngram_range: (3, 3) analyzer: word\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    neautral       0.42      0.02      0.03      8980\n",
            "    negative       0.82      0.20      0.32      9126\n",
            "    positive       0.36      0.99      0.53      8895\n",
            "\n",
            "    accuracy                           0.40     27001\n",
            "   macro avg       0.54      0.40      0.30     27001\n",
            "weighted avg       0.54      0.40      0.29     27001\n",
            "\n",
            "0.5 0\n",
            "ngram_range: (3, 3) analyzer: word\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    neautral       0.55      0.37      0.44      8980\n",
            "    negative       0.76      0.48      0.59      9126\n",
            "    positive       0.52      0.89      0.66      8895\n",
            "\n",
            "    accuracy                           0.58     27001\n",
            "   macro avg       0.61      0.58      0.56     27001\n",
            "weighted avg       0.61      0.58      0.56     27001\n",
            "\n",
            "0.5 0\n",
            "ngram_range: (3, 3) analyzer: word\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    neautral       0.55      0.44      0.49      8980\n",
            "    negative       0.75      0.54      0.63      9126\n",
            "    positive       0.58      0.87      0.70      8895\n",
            "\n",
            "    accuracy                           0.61     27001\n",
            "   macro avg       0.63      0.62      0.60     27001\n",
            "weighted avg       0.63      0.61      0.60     27001\n",
            "\n",
            "0.5 0\n",
            "ngram_range: (3, 3) analyzer: word\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    neautral       0.57      0.48      0.52      8980\n",
            "    negative       0.73      0.59      0.65      9126\n",
            "    positive       0.64      0.86      0.73      8895\n",
            "\n",
            "    accuracy                           0.64     27001\n",
            "   macro avg       0.65      0.64      0.64     27001\n",
            "weighted avg       0.65      0.64      0.64     27001\n",
            "\n",
            "0.5 0.01\n",
            "ngram_range: (3, 3) analyzer: word\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    neautral       0.42      0.02      0.03      8980\n",
            "    negative       0.82      0.20      0.32      9126\n",
            "    positive       0.36      0.99      0.53      8895\n",
            "\n",
            "    accuracy                           0.40     27001\n",
            "   macro avg       0.54      0.40      0.30     27001\n",
            "weighted avg       0.54      0.40      0.29     27001\n",
            "\n",
            "0.5 0.01\n",
            "ngram_range: (3, 3) analyzer: word\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    neautral       0.42      0.02      0.03      8980\n",
            "    negative       0.82      0.20      0.32      9126\n",
            "    positive       0.36      0.99      0.53      8895\n",
            "\n",
            "    accuracy                           0.40     27001\n",
            "   macro avg       0.54      0.40      0.30     27001\n",
            "weighted avg       0.54      0.40      0.29     27001\n",
            "\n",
            "0.5 0.01\n",
            "ngram_range: (3, 3) analyzer: word\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    neautral       0.42      0.02      0.03      8980\n",
            "    negative       0.82      0.20      0.32      9126\n",
            "    positive       0.36      0.99      0.53      8895\n",
            "\n",
            "    accuracy                           0.40     27001\n",
            "   macro avg       0.54      0.40      0.30     27001\n",
            "weighted avg       0.54      0.40      0.29     27001\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NZxirpt-vVmH"
      },
      "source": [
        "###CountVectorizer char analyzer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "usCyMo6pvI8s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c2c12227-7711-421a-ff9b-5fae8e518d4b"
      },
      "source": [
        "for ngram_range in generator(2, 6, 6):\n",
        "  count_vectorizer, count_vectorizer_x_train = fit_transform(CountVectorizer(ngram_range=ngram_range, stop_words=stopwords.words(\"russian\"), analyzer=\"char\"))\n",
        "  vectorize_result.append(naive_bayes(count_vectorizer, count_vectorizer_x_train))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ngram_range: (2, 2) analyzer: char\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    neautral       0.53      0.62      0.57      8980\n",
            "    negative       0.68      0.57      0.62      9126\n",
            "    positive       0.75      0.74      0.75      8895\n",
            "\n",
            "    accuracy                           0.64     27001\n",
            "   macro avg       0.65      0.64      0.65     27001\n",
            "weighted avg       0.65      0.64      0.65     27001\n",
            "\n",
            "ngram_range: (2, 3) analyzer: char\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    neautral       0.56      0.67      0.61      8980\n",
            "    negative       0.71      0.59      0.64      9126\n",
            "    positive       0.82      0.79      0.80      8895\n",
            "\n",
            "    accuracy                           0.68     27001\n",
            "   macro avg       0.69      0.68      0.68     27001\n",
            "weighted avg       0.69      0.68      0.68     27001\n",
            "\n",
            "ngram_range: (2, 4) analyzer: char\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    neautral       0.57      0.68      0.62      8980\n",
            "    negative       0.72      0.60      0.65      9126\n",
            "    positive       0.84      0.81      0.82      8895\n",
            "\n",
            "    accuracy                           0.70     27001\n",
            "   macro avg       0.71      0.70      0.70     27001\n",
            "weighted avg       0.71      0.70      0.70     27001\n",
            "\n",
            "ngram_range: (2, 5) analyzer: char\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    neautral       0.57      0.69      0.63      8980\n",
            "    negative       0.73      0.60      0.66      9126\n",
            "    positive       0.84      0.82      0.83      8895\n",
            "\n",
            "    accuracy                           0.70     27001\n",
            "   macro avg       0.72      0.71      0.71     27001\n",
            "weighted avg       0.72      0.70      0.71     27001\n",
            "\n",
            "ngram_range: (3, 3) analyzer: char\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    neautral       0.56      0.68      0.62      8980\n",
            "    negative       0.72      0.59      0.65      9126\n",
            "    positive       0.83      0.80      0.81      8895\n",
            "\n",
            "    accuracy                           0.69     27001\n",
            "   macro avg       0.70      0.69      0.69     27001\n",
            "weighted avg       0.70      0.69      0.69     27001\n",
            "\n",
            "ngram_range: (3, 4) analyzer: char\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    neautral       0.57      0.69      0.62      8980\n",
            "    negative       0.73      0.60      0.66      9126\n",
            "    positive       0.84      0.82      0.83      8895\n",
            "\n",
            "    accuracy                           0.70     27001\n",
            "   macro avg       0.71      0.70      0.70     27001\n",
            "weighted avg       0.71      0.70      0.70     27001\n",
            "\n",
            "ngram_range: (3, 5) analyzer: char\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    neautral       0.58      0.69      0.63      8980\n",
            "    negative       0.73      0.61      0.66      9126\n",
            "    positive       0.85      0.82      0.83      8895\n",
            "\n",
            "    accuracy                           0.71     27001\n",
            "   macro avg       0.72      0.71      0.71     27001\n",
            "weighted avg       0.72      0.71      0.71     27001\n",
            "\n",
            "ngram_range: (4, 4) analyzer: char\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    neautral       0.57      0.69      0.63      8980\n",
            "    negative       0.73      0.61      0.66      9126\n",
            "    positive       0.84      0.82      0.83      8895\n",
            "\n",
            "    accuracy                           0.70     27001\n",
            "   macro avg       0.72      0.70      0.71     27001\n",
            "weighted avg       0.71      0.70      0.71     27001\n",
            "\n",
            "ngram_range: (4, 5) analyzer: char\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    neautral       0.58      0.69      0.63      8980\n",
            "    negative       0.73      0.61      0.67      9126\n",
            "    positive       0.85      0.83      0.84      8895\n",
            "\n",
            "    accuracy                           0.71     27001\n",
            "   macro avg       0.72      0.71      0.71     27001\n",
            "weighted avg       0.72      0.71      0.71     27001\n",
            "\n",
            "ngram_range: (5, 5) analyzer: char\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    neautral       0.58      0.70      0.63      8980\n",
            "    negative       0.74      0.61      0.67      9126\n",
            "    positive       0.85      0.83      0.84      8895\n",
            "\n",
            "    accuracy                           0.71     27001\n",
            "   macro avg       0.72      0.71      0.72     27001\n",
            "weighted avg       0.72      0.71      0.71     27001\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ricIhkFvmsmB"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q9Pbcho3s-bZ"
      },
      "source": [
        "###Вывод"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "up3UrIVL51dA"
      },
      "source": [
        "pattern = re.compile(r\"[a-zA-Z]+Vectorizer|'word'|'char'|\\([0-9,]+,[ 0-9]+\\)|min_df=[0-9.]|max_df=[0-9.]+|features=[0-9]+\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 358
        },
        "id": "5vyiP5sHxPi_",
        "outputId": "56940df5-6414-447a-dc24-fce993524780"
      },
      "source": [
        "raw_data = []\n",
        "for vectorize in vectorize_result:\n",
        "  score = vectorize[0][\"weighted avg\"]\n",
        "  names_with_params = pattern.findall(str(vectorize[1]))\n",
        "  raw_data.append({\"Vectorizer\": names_with_params[0], \"Analyzer\": names_with_params[1], \"Parameters\": names_with_params[2:], \"Precision\": score[\"precision\"], \"Recall\": score[\"recall\"], \"F1-Score\": score['f1-score']})\n",
        "data_result = pd.DataFrame(raw_data)\n",
        "data_result.sort_values(by=[\"Precision\"], ascending=False).head(10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Vectorizer</th>\n",
              "      <th>Analyzer</th>\n",
              "      <th>Parameters</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1-Score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>55</th>\n",
              "      <td>CountVectorizer</td>\n",
              "      <td>'char'</td>\n",
              "      <td>[max_df=1.0, min_df=1, (5, 5)]</td>\n",
              "      <td>0.723708</td>\n",
              "      <td>0.713122</td>\n",
              "      <td>0.714821</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>54</th>\n",
              "      <td>CountVectorizer</td>\n",
              "      <td>'char'</td>\n",
              "      <td>[max_df=1.0, min_df=1, (4, 5)]</td>\n",
              "      <td>0.719849</td>\n",
              "      <td>0.709122</td>\n",
              "      <td>0.710857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>TfidfVectorizer</td>\n",
              "      <td>'word'</td>\n",
              "      <td>[max_df=0.5, features=32768, min_df=0, (2, 2)]</td>\n",
              "      <td>0.717944</td>\n",
              "      <td>0.714418</td>\n",
              "      <td>0.714325</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>TfidfVectorizer</td>\n",
              "      <td>'word'</td>\n",
              "      <td>[max_df=0.1, features=32768, min_df=0, (2, 2)]</td>\n",
              "      <td>0.717944</td>\n",
              "      <td>0.714418</td>\n",
              "      <td>0.714325</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>52</th>\n",
              "      <td>CountVectorizer</td>\n",
              "      <td>'char'</td>\n",
              "      <td>[max_df=1.0, min_df=1, (3, 5)]</td>\n",
              "      <td>0.717305</td>\n",
              "      <td>0.705937</td>\n",
              "      <td>0.707838</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>TfidfVectorizer</td>\n",
              "      <td>'word'</td>\n",
              "      <td>[max_df=0.1, features=32768, min_df=0, (2, 3)]</td>\n",
              "      <td>0.716079</td>\n",
              "      <td>0.712233</td>\n",
              "      <td>0.711735</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>TfidfVectorizer</td>\n",
              "      <td>'word'</td>\n",
              "      <td>[max_df=0.5, features=32768, min_df=0, (2, 3)]</td>\n",
              "      <td>0.716079</td>\n",
              "      <td>0.712233</td>\n",
              "      <td>0.711735</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49</th>\n",
              "      <td>CountVectorizer</td>\n",
              "      <td>'char'</td>\n",
              "      <td>[max_df=1.0, min_df=1, (2, 5)]</td>\n",
              "      <td>0.715904</td>\n",
              "      <td>0.704122</td>\n",
              "      <td>0.706116</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>53</th>\n",
              "      <td>CountVectorizer</td>\n",
              "      <td>'char'</td>\n",
              "      <td>[max_df=1.0, min_df=1, (4, 4)]</td>\n",
              "      <td>0.714869</td>\n",
              "      <td>0.704011</td>\n",
              "      <td>0.705795</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>51</th>\n",
              "      <td>CountVectorizer</td>\n",
              "      <td>'char'</td>\n",
              "      <td>[max_df=1.0, min_df=1, (3, 4)]</td>\n",
              "      <td>0.712026</td>\n",
              "      <td>0.700604</td>\n",
              "      <td>0.702500</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         Vectorizer Analyzer  ...    Recall  F1-Score\n",
              "55  CountVectorizer   'char'  ...  0.713122  0.714821\n",
              "54  CountVectorizer   'char'  ...  0.709122  0.710857\n",
              "18  TfidfVectorizer   'word'  ...  0.714418  0.714325\n",
              "12  TfidfVectorizer   'word'  ...  0.714418  0.714325\n",
              "52  CountVectorizer   'char'  ...  0.705937  0.707838\n",
              "24  TfidfVectorizer   'word'  ...  0.712233  0.711735\n",
              "30  TfidfVectorizer   'word'  ...  0.712233  0.711735\n",
              "49  CountVectorizer   'char'  ...  0.704122  0.706116\n",
              "53  CountVectorizer   'char'  ...  0.704011  0.705795\n",
              "51  CountVectorizer   'char'  ...  0.700604  0.702500\n",
              "\n",
              "[10 rows x 6 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 129
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nH-mNsWmBrON"
      },
      "source": [
        "Самым лучшим векторайзером стал **CountVectorizer с char** анализатором с гиперпараметром *ngram=(5, 5)*. Также неплохие показатели показывает **TfidfVectorizer** с параметрами *max_df=0.5* *min_df=0* *features=32768* *ngram=(2, 2)*. **CountVectorizer с word** анализатором в первую десятку не попал("
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5QYTwyMtWhAZ"
      },
      "source": [
        "## Задание 5.2 Регулярные выражения\n",
        "\n",
        "Регулярные выражения - способ поиска и анализа строк. Например, можно понять, какие даты в наборе строк представлены в формате DD/MM/YYYY, а какие - в других форматах. \n",
        "\n",
        "Или бывает, например, что перед работой с текстом, надо почистить его от своеобразного мусора: упоминаний пользователей, url и так далее.\n",
        "\n",
        "Навык полезный, давайте в нём тоже потренируемся.\n",
        "\n",
        "Для работы с регулярными выражениями есть библиотека **re**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VaUW5S4gWhAb"
      },
      "source": [
        "import re"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D6aYh7Osl8xr"
      },
      "source": [
        "В регулярных выражениях, кроме привычных символов-букв, есть специальные символы:\n",
        "* **?а** - ноль или один символ **а**\n",
        "* **+а** - один или более символов **а**\n",
        "* **\\*а** - ноль или более символов **а** (не путать с +)\n",
        "* **.** - любое количество любого символа\n",
        "\n",
        "Пример:\n",
        "Выражению \\*a?b. соответствуют последовательности a, ab, abc, aa, aac НО НЕ abb!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q7zOFFA3l_KQ"
      },
      "source": [
        "Рассмотрим подробно несколько наиболее полезных функций:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DbJrUpARWhAd"
      },
      "source": [
        "### findall\n",
        "возвращает список всех найденных непересекающихся совпадений.\n",
        "\n",
        "Регулярное выражение **ab+c.**: \n",
        "* **a** - просто символ **a**\n",
        "* **b+** - один или более символов **b**\n",
        "* **c** - просто символ **c**\n",
        "* **.** - любой символ\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2athHzKuWhAd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a1c022b-62fa-4279-f501-289a526bf100"
      },
      "source": [
        "result = re.findall('ab+c.', 'abcdefghijkabcabcxabc') \n",
        "print(result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['abcd', 'abca']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A9FpIw5RWhAf"
      },
      "source": [
        "Вопрос на внимательность: почему нет abcx?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B5ttzoxEWhAg"
      },
      "source": [
        "**Задание**: вернуть список первых двух букв каждого слова в строке, состоящей из нескольких слов."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GOVKC_7Fv4Zr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64710629-4ba8-442a-e201-26f8b3f85c9b"
      },
      "source": [
        "text = \"Уважаемые коллеги, добрый день!\"\n",
        "re.findall(r\"\\b\\w{2}\", text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Ув', 'ко', 'до', 'де']"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MI18l-l9WhAk"
      },
      "source": [
        "### split\n",
        "разделяет строку по заданному шаблону\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sVKdRoc1WhAl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "92140f38-76dc-4d04-d97d-9035620d0364"
      },
      "source": [
        "result = re.split(',', 'itsy, bitsy, teenie, weenie') \n",
        "print(result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['itsy', ' bitsy', ' teenie', ' weenie']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "10u5efuSWhAm"
      },
      "source": [
        "можно указать максимальное количество разбиений"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9U9EQZMwWhAn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dbc19c13-e3dd-4070-d6bb-e498c816607f"
      },
      "source": [
        "result = re.split(',', 'itsy, bitsy, teenie, weenie', maxsplit=2) \n",
        "print(result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['itsy', ' bitsy', ' teenie, weenie']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0EMcMyflWhAp"
      },
      "source": [
        "**Задание**: разбейте строку, состоящую из нескольких предложений, по точкам, но не более чем на 3 предложения."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dVgPSjEOWhAp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "74d0f28d-48c3-42b2-ccf0-372cd2f39c03"
      },
      "source": [
        "text = \"Уважаемые студенты! Мы на первом занятии специально создали тему, чтобы вы в неё загружали свои работы в отдельные followup discussions.\"\\\n",
        "       \"Не надо создавать новые темы типа «моя работа» или «лаба 1».\"\\\n",
        "       \"Вот вы представьте, мне сейчас 270 человек создадут тут каждый по такой теме под каждую из своих работ.\"\\\n",
        "       \"Форум тут же превратиться в помойку, чтобы этого избежать, все такие темы будут удалятся мною.\"\\\n",
        "       \"При повторных нарушениях буду снижать баллы за низкий уровень цифровой культуры и неспособность к грамотной комммуникации в Интернете.\"\n",
        "\n",
        "re.split(r\"\\.\", text, maxsplit=2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Уважаемые студенты! Мы на первом занятии специально создали тему, чтобы вы в неё загружали свои работы в отдельные followup discussions',\n",
              " 'Не надо создавать новые темы типа «моя работа» или «лаба 1»',\n",
              " 'Вот вы представьте, мне сейчас 270 человек создадут тут каждый по такой теме под каждую из своих работ.Форум тут же превратиться в помойку, чтобы этого избежать, все такие темы будут удалятся мною.При повторных нарушениях буду снижать баллы за низкий уровень цифровой культуры и неспособность к грамотной комммуникации в Интернете.']"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1wrEGqBSWhAr"
      },
      "source": [
        "### sub\n",
        "ищет шаблон в строке и заменяет все совпадения на указанную подстроку\n",
        "\n",
        "параметры: (pattern, repl, string)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "az3KxKWwWhAr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc803506-1bed-4f7c-fca2-5587d5be1f9d"
      },
      "source": [
        "result = re.sub('a', 'b', 'abcabc')\n",
        "print (result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bbcbbc\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qD0n7_HPWhAt"
      },
      "source": [
        "**Задание**: напишите регулярное выражение, которое позволит заменить все цифры в строке на \"DIG\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s_Sdu7xlWhAu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "98febf83-110a-4348-9e9d-a629523bd643"
      },
      "source": [
        "text = \"284189 Папикян Сергей Седракович M33011\"\n",
        "re.sub(r\"\\d\", \"DIG\", text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'DIGDIGDIGDIGDIGDIG Папикян Сергей Седракович MDIGDIGDIGDIGDIG'"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8__oi1PWhAv"
      },
      "source": [
        "**Задание**: напишите  регулярное выражение, которое позволит убрать url из строки."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KwNS9zt4WhAv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "c54ede47-2eea-4d8d-f360-ee225e6b388b"
      },
      "source": [
        "text = \"Мой ИСУ: https://isu.ifmo.ru/pls/apex/f?p=2437:7:108188039133937:::::, Cайт Факультета: https://fitp.itmo.ru/, ПСЖ: https://neerc.ifmo.ru/lgd.pdf\"\n",
        "re.sub(r\"http[s]?://[\\w.:?/=]+\", \"\", text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Мой ИСУ: , Cайт Факультета: , ПСЖ: '"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gStgBJy2WhAx"
      },
      "source": [
        "### compile\n",
        "компилирует регулярное выражение в отдельный объект"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JstTupisWhAy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f4bd803d-e4f3-4ab7-e38a-be5b8eb9740c"
      },
      "source": [
        "# Пример: построение списка всех слов строки:\n",
        "prog = re.compile('[А-Яа-яё\\-]+')\n",
        "prog.findall(\"Слова? Да, больше, ещё больше слов! Что-то ещё.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Слова', 'Да', 'больше', 'ещё', 'больше', 'слов', 'Что-то', 'ещё']"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WXEXc3G0WhA2"
      },
      "source": [
        "**Задание**: для выбранной строки постройте список слов, которые длиннее трех символов."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nFvnIWbUWhA2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e564d8c3-0fab-4dfc-b1fe-838c2f03bf1b"
      },
      "source": [
        "text = \"Выбирай ИТМО - и не выбирай вообще\"\n",
        "pattern = re.compile(r\"\\w{4,}\")\n",
        "pattern.findall(text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Выбирай', 'ИТМО', 'выбирай', 'вообще']"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SQDNZ3HQWhA3"
      },
      "source": [
        "**Задание**: вернуть список доменов (@gmail.com) из списка адресов электронной почты:\n",
        "\n",
        "```\n",
        "abc.test@gmail.com, xyz@test.in, test.first@analyticsvidhya.com, first.test@rest.biz\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ir6pWwVoMru4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2409c411-bbdd-4f13-8501-96435a1a1662"
      },
      "source": [
        "text = \"abc.test@gmail.com, xyz@test.in, test.first@analyticsvidhya.com, first.test@rest.biz\"\n",
        "pattern = re.compile(r\"@[\\w.]+\")\n",
        "pattern.findall(text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['@gmail.com', '@test.in', '@analyticsvidhya.com', '@rest.biz']"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    }
  ]
}